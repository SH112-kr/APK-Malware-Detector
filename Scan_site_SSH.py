from random import expovariate
from urllib import parse




class AutoExtractor:
    def __init__(self):
        import re, pickle, os
        self.reID = re.compile(''' id=("[^"']+"|'[^']+')''', re.I)
        self.reClass = re.compile(''' class=("[^"']+"|'[^']+')''', re.I)
        self.reIdClass = re.compile(''' (id|class)=("[^"']+"|'[^']+')''', re.I)
        self.reNewline = re.compile('[\r\n]{2,}')
        self.templateCachePath = 'elementTemplate/'
        self.commonThreshold = 0.4
        self.maxLookup = 10
        try: os.mkdir(self.templateCachePath)
        except: pass
        self.elementTemplateCache = {}


    def fetchHTML(self, url):
        '''웹 페이지를 가져오는 함수'''
        import urllib, urllib.request, urllib.parse, random
        try:
            t = urllib.parse.urlparse(url)
            params = [(k, v[0]) for k, v in urllib.parse.parse_qs(t.query).items()]
            url = t.scheme + '://' + t.netloc + t.path + '?' + urllib.parse.urlencode(params)
            headerList = ['Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.92 Safari/537.36',
                        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36',
                        'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko']
            try: 
                req = urllib.request.Request(
                    url,
                    headers={
                    'User-Agent': random.sample(headerList, 1)[0]
                    }
                )
                
                f = urllib.request.urlopen(req,data=None,timeout=4)
                newUrl = f.geturl()
                rawbytes = f.read()
                f.close()
            except:
                pass
            try:
                return self.refineDuplicatedHTMLId(rawbytes.decode('utf-8')), newUrl
            except UnicodeDecodeError:
                return self.refineDuplicatedHTMLId(rawbytes.decode('euc-kr')), newUrl
        except:
            print("Site 접속 불가")
            pass

    def refineDuplicatedHTMLId(self, html):
        '''일부 비표준 사이트에서는 엘리먼트 ID를 중복해서 사용합니다. 
        이 경우 각 ID를 유니크하게 바꿔주는 함수'''
        import re
        dups = {}
        from collections import Counter
        for k, v in Counter(id[1:-1] for id in self.reID.findall(html)).most_common():
            if v <= 1: break
            dups[k] = v
        for id, n in dups.items():
            for i in range(n):
                html = re.sub(''' id=('%s'|"%s")''' % (id, id), ' id="%s___%d"' % (id, i), html, 1)
        return html
    
    def waitOpenFile(self, path):
        import os.path, time
        if not os.path.exists(path) or not os.path.isfile(path): return
        for _ in range(10):
            try:
                f = open(path, 'rb')
                return f
            except IOError:
                time.sleep(10)
        raise IOError("Failed to open '%s'" % path)   

        
    def getElementTemplate(self, url, maxLookup = 0):
        '''본문 템플릿을 캐싱하고 가져오는 함수'''
        import time, pickle, os, re
        if maxLookup == 0: maxLookup = self.maxLookup
        # first, search in memory
        for k, v in self.elementTemplateCache.items():
            if url.startswith(k): return v
 
        # second, search in local file
        try:
            nurl = re.sub('[:/]', '_', url)
            for name in os.listdir(self.templateCachePath):
                if nurl.startswith(name):
                    with self.waitOpenFile(self.templateCachePath + name) as f:
                        base, elements = pickle.load(f)
                        self.elementTemplateCache[base] = elements
                        return elements
        except: pass
 
        # else, build and save template
        

    def extractText(self, url):
        '''위의 함수들을 종합하여, 실제 본문을 추출해주는 함수'''
        import re
        import bs4
        try:
            html, url = self.fetchHTML(url)
            soup = bs4.BeautifulSoup(html, "lxml")
            tmplt = self.getElementTemplate(url)
            [s.extract() for s in soup('script')]
            [s.extract() for s in soup('style')]
            [s.extract() for s in soup('iframe')]
            [s.extract() for s in soup('a')]
            [s.extract() for s in soup('li')]
            [s.extract() for s in soup('header')]
            [s.extract() for s in soup('footer')]
            text1 = re.sub("&nbsp; | &nbsp;|\n|\t|\r",'',soup.text)
            text2 = re.sub('\n\n','',text1)
        except:
            pass
            return "x","x"
        return self.reNewline.sub('\n', text2.strip()), url


    

    def scanSiteMain(url):
        import re
        import site_list
        from urllib.parse import urlparse
        
        
        
        tag_list = ['BANK','기관사칭','택배']
        if "http" not in url:
            url = "http://" + url
        uri = url
        try:
            ae = AutoExtractor()
            txt, url = ae.extractText(uri) #분석대상 URL 기입
            if txt == 'x':
                print("사이트 연결 불가")
            
            site_text = re.sub('\s+', ' ', txt)
            list_site_text = site_text.split()
            choice_box = []
            for i in list_site_text:
                for a in tag_list:
                    for key, value in site_list.SITE_LIST[a].items():
                        for one_value in value:
                            if one_value in i:
                                choice_box.append(key)
        #print(choice_box)
            if len(choice_box) > 0:
                max_num = max(set(choice_box),key=choice_box.count)
                print(max_num)
                return max_num
        except: 
            return "에러"
                
                

                




"""    
if __name__ == "__main__":
    import sys, re, time, traceback
    import site_list
    import socket
    import pygeoip
    from urllib.parse import urlparse
    import os
    tag_list = ['BANK','기관사칭','택배']
    uri = 'http://www.naver.com'
    try:
        ae = AutoExtractor()
        txt, url = ae.extractText(uri) #분석대상 URL 기입
        if txt == 'x':
            print("사이트 연결 불가")
            
        site_text = re.sub('\s+', ' ', txt)
        list_site_text = site_text.split()
        choice_box = []
        for i in list_site_text:
            for a in tag_list:
                for key, value in site_list.SITE_LIST[a].items():
                    for one_value in value:
                        if one_value in i:
                            choice_box.append(key)
        #print(choice_box)
        if len(choice_box) > 0:
            max_num = max(set(choice_box),key=choice_box.count)
            #print(max_num,'사칭앱')
            socket_url = urlparse(uri)
            socket_ip = socket.gethostbyname(socket_url.hostname)
            os.chdir("C:\\Users\\SSH\\Desktop\\python_code\\andro_guard")
            geo = pygeoip.GeoIP('GeoLiteCity.dat')
            ip_info = geo.record_by_name(socket_ip)
            country = ip_info['country_code']
            if country != 'KR':
                print(max_num,'사칭')
            else:
                print("1차 안전 판단")
        else:
            print("기타 사칭")
        
    except:
        pass
    
"""